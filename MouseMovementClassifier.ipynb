{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MouseMovementClassifier\n",
    "\n",
    "The goal of this Notebook is to classify users based on their mouse movements. In the \"data\" directory you will find .csv files that contain Y coordinates that describe how a user moved his mouse in order to press a button. The file names can be used as labels for the data. \n",
    "\n",
    "For the sake of simplicity, we are omitting X coordinates. We will use simple Deep Neural Networks to classify users. We can discuss some ways to improve our data collection and classification afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(\"./data\"):\n",
    "    \n",
    "        for filename in files:\n",
    "            with open(\"./data/\" + filename) as csv_file:\n",
    "\n",
    "                csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "                for row in csv_reader:\n",
    "                    inputs.append(transform_input([int(elem) for elem in row]))\n",
    "                    labels.append(filename.replace('.csv', ''))\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "\n",
    "In order to classify users based on our data, we need to extract meaningful features. We could treat the sample data as a sequence but Deep Neural Networks are not suitable for such things (Convolutional or Recurrent Neural Networks would be better for this task).\n",
    "\n",
    "We will try to come up with some features that describe a user's movement in general. \n",
    "\n",
    "For example: We could use some common measures such as mean or median. Furthermore, we could try to split each data entry in two halfs to capture more information about the movement style towards the first and second half. \n",
    "\n",
    "You can edit the function below if you want. \"transform_input\" is called while the data is being read in \"get_data\" (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify features if needed\n",
    "\n",
    "def transform_input(input):\n",
    "    output = []\n",
    "    middle = round(len(input)/2)\n",
    "    first_half = input[middle:]\n",
    "    second_half = input[:middle]\n",
    "    \n",
    "    # initial position\n",
    "    output.append(input[0])\n",
    "    \n",
    "    # end position\n",
    "    output.append(input[-1])\n",
    "    \n",
    "    # number of movements\n",
    "    output.append(len(first_half))\n",
    "    output.append(len(second_half)) \n",
    "    \n",
    "    # highest Y\n",
    "    output.append(max(first_half)) \n",
    "    output.append(max(second_half)) \n",
    "    \n",
    "    # lowest Y\n",
    "    output.append(min(first_half)) \n",
    "    output.append(min(second_half)) \n",
    "    \n",
    "    # avg Y\n",
    "    output.append(np.mean(first_half))\n",
    "    output.append(np.mean(second_half))\n",
    "    \n",
    "    # median Y\n",
    "    output.append(np.median(first_half))\n",
    "    output.append(np.median(second_half))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries that we are using require us to encode our labels. Furthermore we are using a MinMaxScaler to normalize the inputs. This reduces bias due to very large or small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels)\n",
    "    encoded_labels = label_encoder.transform(labels)\n",
    "    one_hot_labels = to_categorical(encoded_labels)\n",
    "    users = label_encoder.classes_.tolist()\n",
    "    print(users)\n",
    "    \n",
    "    return one_hot_labels, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_inputs(inputs):\n",
    "\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    scaled_inputs = min_max_scaler.fit_transform(inputs)\n",
    "\n",
    "    return scaled_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "The function below is used to dynamically generate a DNN. We can specify the amount of Neurons, Layers and Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DNN_model(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    optimizer='adam',\n",
    "    dense_neurons=(150,150,150),\n",
    "    dropout=(0.1,0.1,0.1),\n",
    "    model_name='DNN_model'):\n",
    "\n",
    "    model_input = Input(shape=(input_dim,), dtype='float32')\n",
    "    x = Dense(dense_neurons[0], name='dense_1', activation='relu')(model_input)\n",
    "    x = Dropout(dropout[0])(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    layer_count = 2\n",
    "\n",
    "    if len(dense_neurons)>1:\n",
    "        for neurons,dropout in zip(dense_neurons[1:],dropout[1:]):\n",
    "            x = Dense(neurons, name='dense_'+ str(layer_count), activation='relu')(x)\n",
    "            x = Dropout(dropout)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            layer_count +=1\n",
    "\n",
    "    model_output = Dense(output_dim, name='dense_out' ,activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=model_input, outputs=model_output)\n",
    "    model.name = model_name\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,weights,label_encoder,test_inputs,test_labels):\n",
    "    \n",
    "    model.load_weights(weights)\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for i in range(0,len(test_labels)):\n",
    "        pred = label_encoder.classes_[np.argmax(model.predict(test_inputs[None,i]))]\n",
    "        y_pred.append(pred)\n",
    "        true = label_encoder.classes_[np.argmax(test_labels[i])]\n",
    "        y_true.append(true)\n",
    "\n",
    "    max_group = 0\n",
    "    for i,c in Counter(y_true).items():\n",
    "        max_group = max(c,max_group)\n",
    "    print('Most common class: {}'.format(max_group/len(y_true)))\n",
    "    score = model.evaluate(test_inputs, test_labels, verbose=0)\n",
    "    print(\"Accuracy: \", score[1])\n",
    "\n",
    "    cm_labels = label_encoder.classes_.tolist()\n",
    "\n",
    "    x_labels = [l[:7] for l in cm_labels]\n",
    "    cm = confusion_matrix(y_true,y_pred,labels=cm_labels)\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion Matrix')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(x_labels)))\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticks(np.arange(len(cm_labels)))\n",
    "    ax.set_yticklabels(cm_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call defined functions to transform our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = get_data()\n",
    "scaled_inputs = scale_inputs(inputs)\n",
    "encoded_labels, label_encoder = encode_labels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch (optional)\n",
    "\n",
    "In order to find good parameters for our model, we can perform a grid search. Parameter tuning can take a lot of time and we could speed it up significantly through parallel processing (Hello Spark!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_DNN_model,\n",
    "                        input_dim=len(scaled_inputs[0]),\n",
    "                        output_dim=len(encoded_labels[0]))\n",
    "param_grid = dict(epochs=[50],\n",
    "                  batch_size=[8],\n",
    "                  dense_neurons=[(64,),(32,16)],\n",
    "                  dropout=[(0.25,0.25)],\n",
    "                  optimizer=['adam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=4, random_state=42, shuffle=True)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, verbose=2, cv = cv)\n",
    "grid_result = grid.fit(scaled_inputs, encoded_labels, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the final model\n",
    "\n",
    "Once we have a good idea about the parameters that we want to use, we can train and evaluate our final model. It is easily possible to export the final model and deploy it to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(scaled_inputs, \n",
    "                                                                        encoded_labels, \n",
    "                                                                        test_size=0.2, \n",
    "                                                                        random_state=42,\n",
    "                                                                        stratify=labels)\n",
    "model = create_DNN_model(input_dim=len(scaled_inputs[0]),\n",
    "                         output_dim=len(encoded_labels[0]),\n",
    "                         optimizer='adam',\n",
    "                         dense_neurons=(64,),\n",
    "                         dropout=(0.25,),\n",
    "                         model_name='DNN_model')\n",
    "checkpointer = ModelCheckpoint(filepath='best_model.h5', \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "hist = model.fit(train_inputs, \n",
    "                 train_labels,\n",
    "                 batch_size=16,\n",
    "                 epochs=200,\n",
    "                 callbacks=[checkpointer],\n",
    "                 validation_split=0.2,\n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model,'best_model.h5',label_encoder,test_inputs,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we improve our data and classifier?\n",
    "\n",
    "* ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
